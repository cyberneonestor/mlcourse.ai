{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set desired options\n",
    "import pickle\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n",
    "    # Split the data into the training and validation sets\n",
    "    idx = int(round(X.shape[0] * ratio))\n",
    "    # Classifier training\n",
    "    lr = LogisticRegression(C=C, random_state=seed, solver='liblinear').fit(X[:idx, :], y[:idx])\n",
    "    # Prediction for validation set\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    # Calculate the quality\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross_val(X_train, y_train, estimator, n_splits=10):\n",
    "    time_split = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = cross_val_score(estimator, X_train, y_train, cv=time_split, \n",
    "                            scoring='roc_auc', n_jobs=1)\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df_in):\n",
    "    df = pd.DataFrame()\n",
    "    df['morning'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 7 and ts.hour <= 11 else 0)\n",
    "    df['day'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 12 and ts.hour <= 18 else 0)\n",
    "    df['evening'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 19 and ts.hour <= 23 else 0)\n",
    "    df['night'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 0 and ts.hour <= 6 else 0)\n",
    "    \n",
    "    pi = np.pi\n",
    "    df['hour'] = df_in['time1'].apply(lambda ts: ts.hour)\n",
    "    df['hour_sin'] = df['hour'].apply(lambda ts: np.sin(2*pi*ts/24.))\n",
    "    df['hour_cos'] = df['hour'].apply(lambda ts: np.cos(2*pi*ts/24.))\n",
    "    df['hour_sin_cos'] = df['hour_sin'] * df['hour_cos']\n",
    "    df['hour_sin'] = StandardScaler().fit_transform(df['hour_sin'].values.reshape(-1, 1))\n",
    "    df['hour_cos'] = StandardScaler().fit_transform(df['hour_cos'].values.reshape(-1, 1))\n",
    "    df['hour_sin_cos'] = StandardScaler().fit_transform(df['hour_sin_cos'].values.reshape(-1, 1))\n",
    "    df = pd.get_dummies(df, columns=['hour'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df['start_month'] = df_in['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype('float64')\n",
    "    df = pd.get_dummies(df, columns=['start_month'])\n",
    "    df['start_month'] = df_in['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype('float64')\n",
    "    df['start_month'] = StandardScaler().fit_transform(df[['start_month']])\n",
    "    \n",
    "    df['day_of_week'] = df_in['time1'].apply(lambda ts: ts.dayofweek).astype('float64')\n",
    "    df = pd.get_dummies(df, columns=['day_of_week'])\n",
    "    \n",
    "    def session_lenth_intervals(minuts):\n",
    "        if 5 >= minuts:\n",
    "            return 1\n",
    "        elif 10 >= minuts > 5:\n",
    "            return 2\n",
    "        elif 15 >= minuts > 10:\n",
    "            return 3\n",
    "        elif 20 >= minuts > 15:\n",
    "            return 4\n",
    "        elif 25 >= minuts > 20:\n",
    "            return 5\n",
    "        elif 30 >= minuts > 25:\n",
    "            return 6\n",
    "    \n",
    "    df['session_lenth'] = (full_times.max(axis=1) - full_times.min(axis=1)).astype('timedelta64[m]').astype(int)\n",
    "    df['session_lenth'] = df['session_lenth'].apply(lambda minuts: session_lenth_intervals(minuts))\n",
    "    df = pd.get_dummies(df, columns=['session_lenth'])\n",
    "    \n",
    "    #Getting Unique Counts\n",
    "    unique_count = []\n",
    "    for row in full_sites.values:\n",
    "        unique = np.unique(row)\n",
    "        if 0 in unique:\n",
    "            unique_count.append(len(unique) - 1)\n",
    "        else:\n",
    "            unique_count.append(len(unique))\n",
    "    unique_count = np.array(unique_count).reshape(-1,1)\n",
    "    df['unique'] = unique_count \n",
    "    #df['unique'] = StandardScaler().fit_transform(df['unique'].values.reshape(-1, 1))\n",
    "    \n",
    "    times = ['time%s' % i for i in range(1, 11)]\n",
    "    for time in times:\n",
    "        name = 'ps_' + time\n",
    "        df[name] = df_in[time].apply(lambda ts: ts.timestamp() if isinstance(ts, pd.Timestamp) else 0)\n",
    "        df[name] = StandardScaler().fit_transform(df[name].values.reshape(-1, 1))\n",
    "        \n",
    "    def top_sites_count(sites):\n",
    "        sites_count = 0\n",
    "        top_sites = [77, 80, 76, 29, 21, 81, 879, 22, 75, 82, 23, 35, 881, 37, 33, 3000, 733, 30, 78, 941]\n",
    "        for site in sites:\n",
    "            if site in top_sites:\n",
    "                sites_count += 1\n",
    "        return sites_count\n",
    "    \n",
    "    def top_unic_sites_count(sites):\n",
    "        unic_sites_count = 0\n",
    "        top_sites = [77, 80, 76, 29, 21, 81, 879, 22, 75, 82, 23, 35, 881, 37, 33, 3000, 733, 30, 78, 941]\n",
    "        used_sites = []\n",
    "        for site in sites:\n",
    "            if site in top_sites and site not in used_sites:\n",
    "                used_sites.append(site)\n",
    "                unic_sites_count += 1\n",
    "        return unic_sites_count\n",
    "    \n",
    "    sites = ['site%s' % i for i in range(1, 11)]\n",
    "    \n",
    "    df['top_sites'] = df_in[sites].apply(lambda sites: top_sites_count(sites), axis=1)\n",
    "    df['unic_top_sites'] = df_in[sites].apply(lambda sites: top_unic_sites_count(sites), axis=1)\n",
    "    \n",
    "    df['power_top_sites'] = df['top_sites'] * df['unic_top_sites']\n",
    "    \n",
    "    #df = df.drop('top_sites', axis=1)\n",
    "    #df = df.drop('unic_top_sites', axis=1)\n",
    "    \n",
    "    df['top_sites'] = StandardScaler().fit_transform(df['top_sites'].values.reshape(-1, 1))\n",
    "    df['unic_top_sites'] = StandardScaler().fit_transform(df['unic_top_sites'].values.reshape(-1, 1))\n",
    "    df['power_top_sites'] = StandardScaler().fit_transform(df['power_top_sites'].values.reshape(-1, 1))\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_and_df_split(base_sparce_df, new_feature_df, split:int):\n",
    "    X_train = hstack([base_sparce_df[:split,:], new_feature_df[:split]])\n",
    "    X_test = hstack([base_sparce_df[split:,:], new_feature_df[split:]])\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_times(times_df):\n",
    "    delta_time_df = pd.DataFrame()\n",
    "    count_of_colums = len(times_df.columns)\n",
    "    for i in range(1, count_of_colums):\n",
    "        column_name = f'd_time{i}'\n",
    "        delta_time_df[column_name] = times_df.apply(lambda ts_columns: (ts_columns[f'time{i+1}'] - ts_columns[f'time{i}']).total_seconds() if not pd.isnull(ts_columns[f'time{i+1}']) else 0, axis=1)\n",
    "        delta_time_df[column_name] = StandardScaler().fit_transform(delta_time_df[column_name].values.reshape(-1, 1))\n",
    "    return delta_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SITES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test data sets, change paths if needed\n",
    "PATH_TO_DATA = Path(r'D:\\Programming\\DS\\mlcourse\\course\\jupyter_english\\assignments_fall2019\\Alice')\n",
    "\n",
    "times_train = ['time%s' % i for i in range(1, NUM_SITES + 1)]\n",
    "train_df = pd.read_csv(PATH_TO_DATA / 'train_sessions3.csv',\n",
    "                       index_col='session_id', parse_dates=times_train)\n",
    "\n",
    "#times_drop = ['time%s' % i for i in range(11, NUM_SITES + 1)]\n",
    "#train_df = train_df.drop(times_drop, axis=1)\n",
    "#train_df['is_train'] = 1\n",
    "\n",
    "times_test = ['time%s' % i for i in range(1, 11)]\n",
    "test_df = pd.read_csv(PATH_TO_DATA / 'test_sessions3.csv',\n",
    "                      index_col='session_id', parse_dates=times_test)\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, NUM_SITES + 1)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(np.uint16)\n",
    "\n",
    "# Load websites dictionary\n",
    "with open(PATH_TO_DATA / 'site_dic.pkl', \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# Create dataframe for the dictionary\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), \n",
    "                          columns=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# United dataframe of the initial data \n",
    "sites = ['site%s' % i for i in range(1, NUM_SITES + 1)]\n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df], sort=False)\n",
    "full_df[sites] = full_df[sites].fillna(0).astype(np.uint16)\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sites = full_df[sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inverse id _> site mapping\n",
    "id2site = {v:k for (k, v) in site_dict.items()}\n",
    "# we treat site with id 0 as \"unknown\"\n",
    "id2site[0] = 'unknown'\n",
    "\n",
    "full_sites_names = full_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                 ' '.join([id2site[i] for i in row]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "# so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "# to be split into 'mail', 'google' and 'com')\n",
    "\n",
    "vectorizer_params={'ngram_range': (1, 5), \n",
    "                       'max_features': 50000,\n",
    "                       'tokenizer': lambda s: s.split()}\n",
    "\n",
    "vectorizer = CountVectorizer(**vectorizer_params)\n",
    "full_sites_vect = vectorizer.fit_transform(full_sites_names)\n",
    "\n",
    "y_train = train_df['target'].apply(lambda y: 1 if y > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need site visit times for further feature engineering\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "full_times = full_df[times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_df = new_features(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    303094\n",
       "0     33264\n",
       "Name: session_lenth_1, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features_df['session_lenth_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "TSVD = TruncatedSVD(n_components=5, n_iter=7, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "# so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "# to be split into 'mail', 'google' and 'com')\n",
    "\n",
    "vectorizer_params={'ngram_range': (1, 5), \n",
    "                       'max_features': 50000,\n",
    "                       'tokenizer': lambda s: s.split()}\n",
    "\n",
    "vectorizer_sel = TfidfVectorizer(**vectorizer_params)\n",
    "full_sites_vect_sel = vectorizer.fit_transform(full_sites_names)\n",
    "\n",
    "new_full_matrix = pd.DataFrame(TSVD.fit_transform(full_sites_vect_sel), index=full_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_df = pd.concat([new_features_df, new_full_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = add_feature_and_df_split(full_sites_vect, new_features_df, idx_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 1.25230334e-04 9.24880006e-10 ... 6.27017119e-03\n",
      " 6.77251049e-03 7.33349977e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  17\n",
      "Selected Features:  [ True False  True False False False False False  True  True  True False\n",
      " False False  True  True False False False False False False False False\n",
      "  True False  True False False False False False  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True False False  True False False  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False]\n",
      "Feature Ranking:  [ 1 16  1 59 32 40 41 56  1  1  1  7  8 49  1  1  5  4  3 31 39 25 42 51\n",
      "  1 29  1 33 10 11 53  2  1  1 52 34  9 46 38 45 65 60 57 58 61 62 63 64\n",
      " 36 12 30  1  1 43 44  1 48  6  1  1  1 24 55 35 27 18 26 20 15 21 19 28\n",
      " 37 23 47 54 17 14 13 22 50]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model_lr = LogisticRegression(random_state=17, solver='liblinear')\n",
    "\n",
    "df_sel  = new_features_df[:idx_split]\n",
    "\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model_lr, 17)\n",
    "rfe = rfe.fit(df_sel, y_train)\n",
    "# summarize the selection of the attributes\n",
    "\n",
    "print(\"Num Features: \", rfe.n_features_ )\n",
    "print(\"Selected Features: \", rfe.support_ )\n",
    "print(\"Feature Ranking: \", rfe.ranking_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(new_features_df.columns)\n",
    "selected_f = []\n",
    "f_analyze = {}\n",
    "for i in range(len(features)):\n",
    "    f_analyze[features[i]] = rfe.support_[i]\n",
    "    if rfe.support_[i]:\n",
    "        selected_f.append(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'morning': True,\n",
       " 'day': False,\n",
       " 'evening': True,\n",
       " 'night': False,\n",
       " 'hour_sin': False,\n",
       " 'hour_cos': False,\n",
       " 'hour_sin_cos': False,\n",
       " 'hour_7': False,\n",
       " 'hour_8': True,\n",
       " 'hour_9': True,\n",
       " 'hour_10': True,\n",
       " 'hour_11': False,\n",
       " 'hour_12': False,\n",
       " 'hour_13': False,\n",
       " 'hour_14': True,\n",
       " 'hour_15': True,\n",
       " 'hour_16': False,\n",
       " 'hour_17': False,\n",
       " 'hour_18': False,\n",
       " 'hour_19': False,\n",
       " 'hour_20': False,\n",
       " 'hour_21': False,\n",
       " 'hour_22': False,\n",
       " 'hour_23': False,\n",
       " 'start_month_201301.0': True,\n",
       " 'start_month_201302.0': False,\n",
       " 'start_month_201303.0': True,\n",
       " 'start_month_201304.0': False,\n",
       " 'start_month_201305.0': False,\n",
       " 'start_month_201306.0': False,\n",
       " 'start_month_201307.0': False,\n",
       " 'start_month_201308.0': False,\n",
       " 'start_month_201309.0': True,\n",
       " 'start_month_201310.0': True,\n",
       " 'start_month_201311.0': False,\n",
       " 'start_month_201312.0': False,\n",
       " 'start_month_201401.0': False,\n",
       " 'start_month_201402.0': False,\n",
       " 'start_month_201403.0': False,\n",
       " 'start_month_201404.0': False,\n",
       " 'start_month_201405.0': False,\n",
       " 'start_month_201406.0': False,\n",
       " 'start_month_201407.0': False,\n",
       " 'start_month_201408.0': False,\n",
       " 'start_month_201409.0': False,\n",
       " 'start_month_201410.0': False,\n",
       " 'start_month_201411.0': False,\n",
       " 'start_month_201412.0': False,\n",
       " 'start_month': False,\n",
       " 'day_of_week_0.0': False,\n",
       " 'day_of_week_1.0': False,\n",
       " 'day_of_week_2.0': True,\n",
       " 'day_of_week_3.0': True,\n",
       " 'day_of_week_4.0': False,\n",
       " 'day_of_week_5.0': False,\n",
       " 'day_of_week_6.0': True,\n",
       " 'session_lenth_1': False,\n",
       " 'session_lenth_2': False,\n",
       " 'session_lenth_3': True,\n",
       " 'session_lenth_4': True,\n",
       " 'session_lenth_5': True,\n",
       " 'session_lenth_6': False,\n",
       " 'unique': False,\n",
       " 'ps_time1': False,\n",
       " 'ps_time2': False,\n",
       " 'ps_time3': False,\n",
       " 'ps_time4': False,\n",
       " 'ps_time5': False,\n",
       " 'ps_time6': False,\n",
       " 'ps_time7': False,\n",
       " 'ps_time8': False,\n",
       " 'ps_time9': False,\n",
       " 'ps_time10': False,\n",
       " 'top_sites': False,\n",
       " 'unic_top_sites': False,\n",
       " 'power_top_sites': False,\n",
       " 0: False,\n",
       " 1: False,\n",
       " 2: False,\n",
       " 3: False,\n",
       " 4: False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['morning',\n",
       " 'evening',\n",
       " 'hour_8',\n",
       " 'hour_9',\n",
       " 'hour_10',\n",
       " 'hour_14',\n",
       " 'hour_15',\n",
       " 'start_month_201301.0',\n",
       " 'start_month_201303.0',\n",
       " 'start_month_201309.0',\n",
       " 'start_month_201310.0',\n",
       " 'day_of_week_2.0',\n",
       " 'day_of_week_3.0',\n",
       " 'day_of_week_6.0',\n",
       " 'session_lenth_3',\n",
       " 'session_lenth_4',\n",
       " 'session_lenth_5']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df = new_features_df[selected_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = add_feature_and_df_split(full_sites_vect, selected_features_df, idx_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1.668, random_state=17, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.88029659, 0.88074801, 0.96455401, 0.97339456, 0.92665874,\n",
       "        0.97195773, 0.91383227, 0.95852479, 0.90321213, 0.96461873]),\n",
       " 0.9337797556808602)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, logit)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = np.logspace(-2, 2, 10)\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=1, cv=time_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.668, class_weight=None,\n",
       "                                          dual=False, fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=17, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'C': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
       "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
       "       3.59381366e+01, 1.00000000e+02])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9359786820686642,\n",
       " {'C': 0.21544346900318834},\n",
       " LogisticRegression(C=0.21544346900318834, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=17, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_, logit_grid_searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_best = logit_grid_searcher.best_estimator_\n",
    "y_test = logit_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Write it to the file which could be submitted\n",
    "write_to_submission_file(y_test, '19___baseline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

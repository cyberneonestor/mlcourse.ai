{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set desired options\n",
    "import pickle\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n",
    "    # Split the data into the training and validation sets\n",
    "    idx = int(round(X.shape[0] * ratio))\n",
    "    # Classifier training\n",
    "    lr = LogisticRegression(C=C, random_state=seed, solver='liblinear').fit(X[:idx, :], y[:idx])\n",
    "    # Prediction for validation set\n",
    "    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n",
    "    # Calculate the quality\n",
    "    score = roc_auc_score(y[idx:], y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross_val(X_train, y_train, estimator, n_splits=10):\n",
    "    time_split = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = cross_val_score(estimator, X_train, y_train, cv=time_split, \n",
    "                            scoring='roc_auc', n_jobs=1)\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df_in):\n",
    "    df = pd.DataFrame()\n",
    "    df['morning'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 7 and ts.hour <= 11 else 0)\n",
    "    df['day'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 12 and ts.hour <= 18 else 0)\n",
    "    df['evening'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 19 and ts.hour <= 23 else 0)\n",
    "    df['night'] = df_in['time1'].apply(lambda ts: 1 if ts.hour >= 0 and ts.hour <= 6 else 0)\n",
    "    \n",
    "    df['start_month'] = df_in['time1'].apply(lambda ts: 100 * ts.year + ts.month).astype('float64')\n",
    "    df['start_month'] = StandardScaler().fit_transform(df[['start_month']])\n",
    "    \n",
    "    df['day_of_week'] = df_in['time1'].apply(lambda ts: ts.dayofweek).astype('float64')\n",
    "    df = pd.get_dummies(df, columns=['day_of_week'])\n",
    "    \n",
    "    df['session_lenth'] = (full_times.max(axis=1) - full_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n",
    "    df['session_lenth'] = StandardScaler().fit_transform(df['session_lenth'].values.reshape(-1, 1))\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_and_df_split(base_sparce_df, new_feature_df, split:int):\n",
    "    X_train = hstack([base_sparce_df[:split,:], new_feature_df[:split]])\n",
    "    X_test = hstack([base_sparce_df[split:,:], new_feature_df[split:]])\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_times(times_df):\n",
    "    delta_time_df = pd.DataFrame()\n",
    "    count_of_colums = len(times_df.columns)\n",
    "    for i in range(1, count_of_colums):\n",
    "        column_name = f'd_time{i}'\n",
    "        delta_time_df[column_name] = times_df.apply(lambda ts_columns: (ts_columns[f'time{i+1}'] - ts_columns[f'time{i}']).total_seconds() if not pd.isnull(ts_columns[f'time{i+1}']) else 0, axis=1)\n",
    "        delta_time_df[column_name] = StandardScaler().fit_transform(delta_time_df[column_name].values.reshape(-1, 1))\n",
    "    return delta_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test data sets, change paths if needed\n",
    "PATH_TO_DATA = Path(r'D:\\Programming\\DS\\mlcourse\\course\\jupyter_english\\assignments_fall2019\\Alice')\n",
    "\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv(PATH_TO_DATA / 'train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv(PATH_TO_DATA / 'test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# United dataframe of the initial data \n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "full_df[sites] = full_df[sites].fillna(0).astype(np.uint16)\n",
    "\n",
    "# Load websites dictionary\n",
    "with open(PATH_TO_DATA / 'site_dic.pkl', \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# Create dataframe for the dictionary\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), \n",
    "                          columns=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inverse id _> site mapping\n",
    "id2site = {v:k for (k, v) in site_dict.items()}\n",
    "# we treat site with id 0 as \"unknown\"\n",
    "id2site[0] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sites = full_df[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                 ' '.join([id2site[i] for i in row]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "# so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "# to be split into 'mail', 'google' and 'com')\n",
    "\n",
    "vectorizer_params={'ngram_range': (1, 5), \n",
    "                       'max_features': 50000,\n",
    "                       'tokenizer': lambda s: s.split()}\n",
    "\n",
    "vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "full_sites_vect = vectorizer.fit_transform(full_sites)\n",
    "y_train = train_df['target'].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need site visit times for further feature engineering\n",
    "full_times = full_df[times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_df = new_features(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = add_feature_and_df_split(full_sites_vect, new_features_df, idx_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1.668, random_state=17, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.81538376, 0.87141117, 0.87554055, 0.97624506, 0.93791066,\n",
       "        0.97252473, 0.92442347, 0.95966785, 0.7721333 , 0.96758319]),\n",
       " 0.9072823744783095)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, logit)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(7,5,3,), random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 33min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.81870963, 0.78065961, 0.80266712, 0.97692241, 0.87050803,\n",
       "        0.9138019 , 0.9162999 , 0.93049019, 0.56418961, 0.85900021]),\n",
       " 0.843324860270401)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, NN)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.LinearSVC(random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.80630067, 0.85105696, 0.84768611, 0.98543051, 0.92968396,\n",
       "        0.96146408, 0.9372008 , 0.95106432, 0.72142143, 0.9531153 ]),\n",
       " 0.8944424129186173)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, SVM)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 60 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.74509985, 0.72882743, 0.83835484, 0.93183427, 0.85140338,\n",
       "        0.89973064, 0.93029657, 0.92789995, 0.86043524, 0.88407482]),\n",
       " 0.8597956990103931)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, RF)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "SGDC = linear_model.SGDClassifier(max_iter=1000, tol=1e-3, loss='log', random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.78204053, 0.86988416, 0.89456299, 0.93114498, 0.92993962,\n",
       "        0.9567261 , 0.88629256, 0.94955991, 0.71773593, 0.94156989]),\n",
       " 0.8859456684179297)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, SGDC)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "XGB_reg = xgb.XGBRegressor(10, 0.1, 1000, objective= 'binary:logistic', random_state = 17, booster = 'gblinear',scale_pos_weight = 109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.76879199, 0.80577042, 0.77758492, 0.74046828, 0.86718052,\n",
       "        0.85215601, 0.88196134, 0.93701612, 0.7507068 , 0.88289932]),\n",
       " 0.8264535713552238)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, XGB_reg)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_clf = xgb.XGBClassifier (random_state = 17, booster = 'gblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.73739678, 0.79940628, 0.79207199, 0.73970301, 0.86271045,\n",
       "        0.83918773, 0.89495261, 0.92673776, 0.74520682, 0.87186432]),\n",
       " 0.8209237754716991)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, XGB_clf)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = logit\n",
    "clf4 = NN\n",
    "clf2 = RF\n",
    "clf3 = SGDC\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('sgd', clf3)], voting='soft') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_scores = time_cross_val(X_train, y_train, eclf)\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
